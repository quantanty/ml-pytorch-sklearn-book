\subsection{Convolutional Neural Networks}
\begin{itemize}[topsep=0pt]
    \item CNNs mimic the human visual cortex and automatically learn features from raw data.
    \item Feature hierarchies: low-level features $\rightarrow$ high-level representations.
    \item Key ideas: \textbf{sparse connectivity} and \textbf{parameter sharing}.
    \item Pooling layers reduce spatial dimensions and help with generalization.
\end{itemize}

\subsection{Discrete Convolution}
\begin{align*}
y[i] &= \sum_{k=-\infty}^{+\infty} x[i-k] \cdot w[k] \\
Y[i,j] &= \sum_{k_1} \sum_{k_2} X[i-k_1, j-k_2] \cdot W[k_1, k_2]
\end{align*}

\noindent
Important hyperparameters: padding (full, same, valid), stride.

\subsection{Pooling Layers}
\begin{itemize}
    \item Max-pooling and mean-pooling.
    \item Improve robustness to noise, reduce overfitting.
    \item Can use overlapping or non-overlapping pooling.
\end{itemize}

\subsection{Convolution layer}
\begin{itemize}
    \item Handling multiple input/output channels:
\end{itemize}

\begin{align*}
Z^{\text{conv}}[:,:,k] &= \sum_{c=1}^{C_{in}} W[:,:,c,k] * X[:,:,c] \\
Z[:,:,k] &= Z^{\text{conv}} + b[k] \\
A[:,:,k] &= \sigma(Z[:,:,k])
\end{align*}

\subsection{Regularization}
\begin{itemize}
    \item L2 regularization (weight decay) and dropout help prevent overfitting.
    \item Dropout encourages robust feature learning.
\end{itemize}

\subsection{Loss Functions}
\begin{itemize}
    \item Binary classification: \texttt{BCEWithLogitsLoss}, \texttt{BCELoss}
    \item Multiclass: \texttt{CrossEntropyLoss}, \texttt{NLLLoss}
\end{itemize}

\subsection{Other Techniques}
\begin{itemize}
    \item \textbf{Data augmentation}: enhances generalization.
    \item \textbf{Global average pooling}: reduces parameters.
\end{itemize}