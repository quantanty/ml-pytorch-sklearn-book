\subsection{Project one - predicting the sentiment of IMDb reviews}
\subsubsection{IMDb reviews dataset}
The tutorial code loads the IMDb reviews datset using \texttt{torchtext} package. However, \texttt{torchtext} uses \texttt{torchdata} to load data, which doesn't support new python versions. I have to download the dataset from hugging face using pandas.readparquet().

\begin{pythoncode}
splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}
df = pd.read_parquet("hf://datasets/stanfordnlp/imdb/" + splits["train"])

test_dataset = pd.read_parquet("hf://datasets/stanfordnlp/imdb/" + splits["test"])
\end{pythoncode}

Split train dataset to train set and validation set.
\begin{pythoncode}
from sklearn.model_selection import train_test_split
train_dataset, valid_dataset = train_test_split(df, train_size=20000, test_size=5000, random_state=1)
\end{pythoncode}

\subsubsection{Processing the text data}
Tokenization
\begin{pythoncode}
token_counts = Counter()

def tokenizer(text):
    text = re.sub(r'<[^>]*>', '', text)
    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\)|\(|D|P)', text.lower())
    text = re.sub(r'[\W]+', ' ', text.lower()) +\
        ' '.join(emoticons).replace('-', '')
    tokenized = text.split()
    return tokenized

for i, (text, label) in train_dataset.iterrows():
    tokens = tokenizer(text)
    token_counts.update(tokens)
 
print('Vocab-size:', len(token_counts))
\end{pythoncode}

\begin{verbatim}
Vocab-size: 69353
\end{verbatim}

Vocab
\begin{pythoncode}
from torchtext.vocab import vocab
sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)
ordered_dict = OrderedDict(sorted_by_freq_tuples)
vocab = vocab(ordered_dict)
vocab.insert_token("<pad>", 0)
vocab.insert_token("<unk>", 1)
vocab.set_default_index(1)

print([vocab[token] for token in ['this', 'is', 'an', 'example']])
\end{pythoncode}

collate batch function.
\begin{pythoncode}
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def collate_batch(batch):
    label_list, text_list, lengths = [], [], []
    for _text, _label in batch:
        label_list.append(label_pipeine(_label))
        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
        text_list.append(processed_text)
        lengths.append(processed_text.size(0))
    label_list = torch.tensor(label_list)
    lengths = torch.tensor(lengths)
    padded_text_list = nn.utils.rnn.pad_sequence(text_list, batch_first=True)
    return padded_text_list.to(device), label_list.to(device), lengths.to(device)
\end{pythoncode}

\subsubsection{Loading data}
Define TabularDataset class, which can return item from dataframe.
\begin{pythoncode}
class TabularDataset(Dataset):
    def __init__(self, dataframe):
        self.dataframe = dataframe
    
    def __len__(self):
        return len(self.dataframe)
    
    def __getitem__(self, idx):
        return self.dataframe.iloc[idx]['text'], self.dataframe.iloc[idx]['label']
\end{pythoncode}

prepare data loaders.
\begin{pythoncode}
batch_size = 32
train_set = TabularDataset(train_dataset)
valid_set = TabularDataset(valid_dataset)
test_set = TabularDataset(test_dataset)

train_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)
valid_dl = DataLoader(valid_set, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)
test_dl = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)
\end{pythoncode}

\subsection{Project two - character-level language modeling with Pytorch}