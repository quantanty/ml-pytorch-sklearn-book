# Going Deeper - The Mechanics of PyTorch

## Topics in this chapter
- Understanding and working with PyTorch computation graphs
- Working with PyTorch and Tensor objects
- Solving the classic XOR problem and understanding model capacity
- Building complex NN models using Pytorch's `Sequential` class and the `nn.Module` class
- Computing gradients using automatic differentiation and `torch.autograd`


## Theoretical notes
PyTorch performs its computations based on a **directed acyclic graph (DAG)**.

**Xavier initialization**: roughly balance the variance of the gradients across different layers. Otherwise, some layers may get too much attention during training while the other layers lag behind.

Choose the interval of uniform distribution:

$$ \large
W \sim Uniform(-\frac{\sqrt{6}}{\sqrt{n_{in}+n_{out}}}, \frac{\sqrt{6}}{\sqrt{n_{in} + n_{out}}})
$$

Choose the standard deviation of Gaussian:

$$ \large
\sigma = \frac{\sqrt{2}}{\sqrt{n_{in} + n_{out}}}
$$

**Partial derivatives**: $\frac{\partial f}{\partial x_1}$ can be understood as the rate of change of a multivariate function with respect to one of its inputs (here: $x_1$).

**Gradients**: $\nabla f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},...)$ is a vector composed of all the inputs' partial derivatives.

**Adversarial examples**
Computing gradients of the loss with respect to the input example is used for generating adversarial examples (or adversarial attacks). In conputer vision, adversarial examples are examples that are generated by adding some small, imperceptible noise to the input example, which results in a deep NN misclassifying them.

## Learn new classes/methods
- `nn.init.xavier_normal_()`
- `nn.Sequential`
- `mlxtend.plotting.plot_decision_regions()`

## Practice some mini-projects
- Predicting the fuel efficiency of a car
- Classifying MNIST handwritten digits

## Libraries and APIs developed on top of Pytorch
- PyTorch-Lightning
- fastai
- Catalyst
- Pytorch-Ignite


## Related papers/articles
- *Understanding the difficulty of deep feedforward neural networks, Xavier Glorot and Yoshua Bengio*, 2010 ([link](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))

- *Christian Szegedy et al., Intriguing properties of neural networks* ([link](https://arxiv.org/pdf/1312.6199.pdf))