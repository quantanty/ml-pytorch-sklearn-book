\documentclass[12pt]{article}
\usepackage[T5]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
% \usepackage{hyperref}
\usepackage[colorlinks=true,
            linkcolor=blue,
            filecolor=magenta,
            urlcolor=cyan,
            citecolor=green,
            bookmarks=true,
            bookmarksopen=true]{hyperref}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{margin=1in}

\title{Chapter 14 Report: \\ Classifying Images with CNNs}
\author{Quan Pham}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Overview}
In this chapter, I studied convolutional neural networks (CNNs) and their application in image classification tasks. CNNs are designed to extract hierarchical features from images, which makes them highly effective for vision-related problems.

\section{Key Concepts}

\subsection{Convolutional Neural Networks}
\begin{itemize}[topsep=0pt]
    \item CNNs mimic the human visual cortex and automatically learn features from raw data.
    \item Feature hierarchies: low-level features $\rightarrow$ high-level representations.
    \item Key ideas: \textbf{sparse connectivity} and \textbf{parameter sharing}.
    \item Pooling layers reduce spatial dimensions and help with generalization.
\end{itemize}

\subsection{Discrete Convolution}
\begin{align*}
y[i] &= \sum_{k=-\infty}^{+\infty} x[i-k] \cdot w[k] \\
Y[i,j] &= \sum_{k_1} \sum_{k_2} X[i-k_1, j-k_2] \cdot W[k_1, k_2]
\end{align*}

\noindent
Important hyperparameters: padding (full, same, valid), stride.

\subsection{Pooling Layers}
\begin{itemize}
    \item Max-pooling and mean-pooling.
    \item Improve robustness to noise, reduce overfitting.
    \item Can use overlapping or non-overlapping pooling.
\end{itemize}

\subsection{CNN Implementation}
\begin{itemize}
    \item Handling multiple input/output channels:
\end{itemize}

\begin{align*}
Z^{\text{conv}}[:,:,k] &= \sum_{c=1}^{C_{in}} W[:,:,c,k] * X[:,:,c] \\
Z[:,:,k] &= Z^{\text{conv}} + b[k] \\
A[:,:,k] &= \sigma(Z[:,:,k])
\end{align*}

\subsection{Regularization}
\begin{itemize}
    \item L2 regularization (weight decay) and dropout help prevent overfitting.
    \item Dropout encourages robust feature learning.
\end{itemize}

\subsection{Loss Functions}
\begin{itemize}
    \item Binary classification: \texttt{BCEWithLogitsLoss}, \texttt{BCELoss}
    \item Multiclass: \texttt{CrossEntropyLoss}, \texttt{NLLLoss}
\end{itemize}

\subsection{Other Techniques}
\begin{itemize}
    \item \textbf{Data augmentation}: enhances generalization.
    \item \textbf{Global average pooling}: reduces parameters.
\end{itemize}

\section{Practice Notes (To Do)}
\vspace{-0.5em}
\begin{itemize}
    \item [\textbf{--}] Thêm phần báo cáo mô hình: độ chính xác, loss, biểu đồ learning curve.
    \item [\textbf{--}] Ghi lại thông số mạng và kết quả tuning hyperparameters.
    \item [\textbf{--}] Viết nhận xét cá nhân, khó khăn gặp phải khi cài CNN bằng PyTorch.
\end{itemize}

\section{References}
\vspace{-0.5em}
\begin{itemize}
    \item LeCun et al. (1989), \textit{Handwritten Digit Recognition with a Back-Propagation Network}.
    \item Krizhevsky et al. (2012), \textit{ImageNet Classification with Deep CNNs}.
    \item Srivastava et al. (2014), \textit{Dropout: A Simple Way to Prevent Overfitting}.
    \item Kingma and Ba (2014), \textit{Adam: A Method for Stochastic Optimization}.
\end{itemize}

\end{document}
